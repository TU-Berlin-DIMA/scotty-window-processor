package benchmark.jobs;

import benchmark.LoadGeneratorSource;
import benchmark.ThroughputLogger2;
import de.tub.dima.scotty.core.windowType.SlidingWindow;
import de.tub.dima.scotty.core.windowType.TumblingWindow;
import de.tub.dima.scotty.core.windowType.Window;
import de.tub.dima.scotty.stormconnector.demo.PrinterBolt;
import de.tub.dima.scotty.stormconnector.demo.SlidingWindowSumBolt;
import de.tub.dima.scotty.stormconnector.demo.TumblingWindowSumBolt;
import org.apache.storm.Config;
import org.apache.storm.LocalCluster;
import org.apache.storm.topology.TopologyBuilder;
import org.apache.storm.topology.base.BaseWindowedBolt.Duration;
import org.apache.storm.tuple.Fields;
import org.javatuples.Pair;

import java.util.List;

import static java.lang.Math.toIntExact;

public class StormBenchmarkJob {
    public StormBenchmarkJob(List<Window> assigners, long runtime, int throughput, List<Pair<Long, Long>> gaps) {
        LocalCluster cluster = new LocalCluster();
        TopologyBuilder builder = new TopologyBuilder();

        int parallelism_hint = 1;
        Config conf = new Config();
        conf.setDebug(false);
        conf.setNumWorkers(1);
        conf.setMaxTaskParallelism(1);
        //Disable Acking    
        conf.setNumAckers(0);

        builder.setSpout("loadGenerator", new LoadGeneratorSource(runtime, throughput, gaps), parallelism_hint);

        builder.setBolt("throughputLogger", new ThroughputLogger2(), parallelism_hint).shuffleGrouping("loadGenerator");

        int windowNumber = 0;
        for (Window w : assigners) {
            if (w instanceof TumblingWindow) {
                int size = toIntExact(((TumblingWindow) w).getSize());
                conf.setMessageTimeoutSecs(size + size);
                builder.setBolt("tumblingsum" + (++windowNumber), new TumblingWindowSumBolt()
                        .withTimestampField("ts")
                        .withWatermarkInterval(Duration.seconds(1))//1 Sec Watermark
                        .withTumblingWindow(Duration.of(size)), parallelism_hint)
                        .fieldsGrouping("loadGenerator", new Fields("key"));
                //Uncomment to print results
                //builder.setBolt("printer"+windowNumber, new PrinterBolt()).shuffleGrouping("tumblingsum"+windowNumber);
            }
            if (w instanceof SlidingWindow) {
                int size = toIntExact(((SlidingWindow) w).getSize());
                int slide = toIntExact(((SlidingWindow) w).getSlide());
                conf.setMessageTimeoutSecs(slide + size*2);

                builder.setBolt("slidingsum" + (++windowNumber), new SlidingWindowSumBolt()
                        .withTimestampField("ts")
                        .withWatermarkInterval(Duration.seconds(1))//1 Sec Watermark
                        .withWindow(Duration.of(size), Duration.of(slide)), parallelism_hint)
                        .fieldsGrouping("loadGenerator", new Fields("key"));
                //Uncomment to print results
                //builder.setBolt("printer"+windowNumber, new PrinterBolt()).shuffleGrouping("slidingsum"+windowNumber);
            }
        }
        cluster.submitTopology("StormBenchmarkTopology", conf, builder.createTopology());

        //After the topology is submitted, code continues to execute from this line immediately without waiting topology execution
        //We can't kill the topology here because we need to wait enough to have correct throughput mean.

        //killTopology("StormBenchmarkTopology",cluster);

        /* Storm topologies never kill themselves and always waits for the next input tuple!
         *
         * Programmer can kill the topology from a spout/bolt when:
         *  1) Data is no longer generated by the spout
         *  2) Final bolt instance receives a signaling tuple that indicates the end of the stream.
         *
         *  Both cases does not apply to our tests because killing a topology results in not being able to wait enough to reach correct throughput mean.
         *  In order to reach correct mean results we actively wait after the topology submission for an estimate amount of time depending on the workload.
         *
         */

        long endTime = System.currentTimeMillis() + runtime * 2;
        while (System.currentTimeMillis() < endTime) {
            //Actively wait for topology to finish
        }

        //At this moment code returns to the calling "Runner" class and writes the throughput mean to disk.
    }

    public void killTopology(String topoName, LocalCluster cluster) {
        cluster.killTopology(topoName);
        cluster.shutdown();
    }
}

